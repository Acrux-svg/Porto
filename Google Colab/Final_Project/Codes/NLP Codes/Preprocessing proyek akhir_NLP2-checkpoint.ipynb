{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383d256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dcd3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hasil scraping1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0140df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Datetime','Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4936157e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-14 23:59:15+00:00</td>\n",
       "      <td>@Dickystyawan11 @seruanhl @AgusYudhoyono Bangg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-14 23:58:07+00:00</td>\n",
       "      <td>@eg_nrs Saya beli minyak goreng curah di warun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-14 23:56:22+00:00</td>\n",
       "      <td>Kang dia masak nasi goreng guna minyak ron95 h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-14 23:53:54+00:00</td>\n",
       "      <td>@BinPekok @JerapahBZP @bachrum_achmadi @ruhuts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-14 23:50:33+00:00</td>\n",
       "      <td>@KING_RAJA_ID Mandalika bukannya boncos y, mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498269</th>\n",
       "      <td>2020-01-01 01:38:56+00:00</td>\n",
       "      <td>@Mushthofa_22 Page 1 \\nTanganku kecipratan min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498270</th>\n",
       "      <td>2020-01-01 01:34:46+00:00</td>\n",
       "      <td>Dari taon ke taon selalu waspada tuh Cipratan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498271</th>\n",
       "      <td>2020-01-01 00:49:10+00:00</td>\n",
       "      <td>@lliaeka Minyak goreng di dapur banyak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498272</th>\n",
       "      <td>2020-01-01 00:28:06+00:00</td>\n",
       "      <td>Minyak goreng habis, popok habis, indomie habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498273</th>\n",
       "      <td>2020-01-01 00:13:04+00:00</td>\n",
       "      <td>grosir minyak goreng terbaru https://t.co/xGIh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498274 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Datetime  \\\n",
       "0       2022-05-14 23:59:15+00:00   \n",
       "1       2022-05-14 23:58:07+00:00   \n",
       "2       2022-05-14 23:56:22+00:00   \n",
       "3       2022-05-14 23:53:54+00:00   \n",
       "4       2022-05-14 23:50:33+00:00   \n",
       "...                           ...   \n",
       "498269  2020-01-01 01:38:56+00:00   \n",
       "498270  2020-01-01 01:34:46+00:00   \n",
       "498271  2020-01-01 00:49:10+00:00   \n",
       "498272  2020-01-01 00:28:06+00:00   \n",
       "498273  2020-01-01 00:13:04+00:00   \n",
       "\n",
       "                                                     Text  \n",
       "0       @Dickystyawan11 @seruanhl @AgusYudhoyono Bangg...  \n",
       "1       @eg_nrs Saya beli minyak goreng curah di warun...  \n",
       "2       Kang dia masak nasi goreng guna minyak ron95 h...  \n",
       "3       @BinPekok @JerapahBZP @bachrum_achmadi @ruhuts...  \n",
       "4       @KING_RAJA_ID Mandalika bukannya boncos y, mas...  \n",
       "...                                                   ...  \n",
       "498269  @Mushthofa_22 Page 1 \\nTanganku kecipratan min...  \n",
       "498270  Dari taon ke taon selalu waspada tuh Cipratan ...  \n",
       "498271             @lliaeka Minyak goreng di dapur banyak  \n",
       "498272  Minyak goreng habis, popok habis, indomie habi...  \n",
       "498273  grosir minyak goreng terbaru https://t.co/xGIh...  \n",
       "\n",
       "[498274 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9408ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=6c8fd2fc7ee5d3fca47c48cd8846478ae094660095b0119a8cd5735db7137956\n",
      "  Stored in directory: c:\\users\\mocha\\appdata\\local\\pip\\cache\\wheels\\fa\\7a\\e9\\22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66aef1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from transformers) (3.4.2)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mocha\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.7.0 tokenizers-0.12.1 transformers-4.19.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mocha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e7317",
   "metadata": {},
   "source": [
    "# Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c73ec496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Buat fungsi untuk langkah case folding\n",
    "def casefolding(text):\n",
    "   # ubah text menjadi huruf kecil\n",
    "  text = text.lower()\n",
    "  # ubah enter menjadi spasi\n",
    "  text = re.sub(r'\\n', ' ', text)\n",
    "  # hapus emoji\n",
    "  text = emoji.demojize(text)\n",
    "  text = re.sub(':[A-Za-z_-]+:', ' ', text) # delete emoji\n",
    "  # hapus emoticon\n",
    "  text = re.sub(r\"([xX;:]'?[dDpPvVoO3)(])\", ' ', text)\n",
    "  # hapus link\n",
    "  text = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\", \"\", text)\n",
    "  # hapus usename\n",
    "  text = re.sub(r\"@[^\\s]+[\\s]?\", ' ', text)\n",
    "  # hapus hashtag\n",
    "  text = re.sub(r'#(\\S+)', r'\\1', text)\n",
    "  # hapus angka dan beberapa simbol\n",
    "  text = re.sub('[^a-zA-Z,.?!]+',' ',text)\n",
    "  # clear spasi\n",
    "  text = re.sub('[ ]+',' ',text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a573c37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data\t:  @Dickystyawan11 @seruanhl @AgusYudhoyono Bangga rakyatnya ngantri minyak goreng,,ðŸ¤£ðŸ¤£\n",
      "Case folding\t:   bangga rakyatnya ngantri minyak goreng,, \n"
     ]
    }
   ],
   "source": [
    "raw_sample = data['Text'].iloc[0]\n",
    "case_folding = casefolding(raw_sample)\n",
    "\n",
    "print('Raw data\\t: ', raw_sample)\n",
    "print('Case folding\\t: ', case_folding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1d6b9",
   "metadata": {},
   "source": [
    "# Word Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76235e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 28, saw 411\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6364/2411401695.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkey_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://github.com/louisowen6/NLP_bahasa_resources/blob/master/combined_slang_words.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1248\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m                 \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 28, saw 411\n"
     ]
    }
   ],
   "source": [
    "key_norm = pd.read_csv('https://github.com/louisowen6/NLP_bahasa_resources/blob/master/combined_slang_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a29b5667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>singkat</th>\n",
       "      <th>hasil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>@</td>\n",
       "      <td>di</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ad</td>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>adlh</td>\n",
       "      <td>adalah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>afaik</td>\n",
       "      <td>as far as i know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>NaN</td>\n",
       "      <td>wkwkkw</td>\n",
       "      <td>tertawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ahokncc</td>\n",
       "      <td>ahok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>istaa</td>\n",
       "      <td>nista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>NaN</td>\n",
       "      <td>benarjujur</td>\n",
       "      <td>jujur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mgkin</td>\n",
       "      <td>mungkin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1021 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      _id         singkat                 hasil\n",
       "0      NaN             @                   di  \n",
       "1      NaN          abis               habis   \n",
       "2      NaN            ad                  ada  \n",
       "3      NaN          adlh               adalah  \n",
       "4      NaN         afaik     as far as i know  \n",
       "...    ...            ...                   ...\n",
       "1016   NaN        wkwkkw             tertawa   \n",
       "1017   NaN       ahokncc                 ahok  \n",
       "1018   NaN         istaa                nista  \n",
       "1019   NaN    benarjujur                jujur  \n",
       "1020   NaN         mgkin               mungkin \n",
       "\n",
       "[1021 rows x 3 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_norm = pd.read_csv('combined_slang_words.txt')\n",
    "key_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f38f2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_norm['_id '] = np.arange(key_norm.shape[0])\n",
    "key_norm['_id ']= key_norm['_id '].astype(\"int32\")\n",
    "def add_1(x):\n",
    "    return x+1\n",
    "key_norm['_id '] = key_norm['_id '].add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "06acda78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id         int32\n",
      "singkat    object\n",
      " hasil     object\n",
      "dtype: object\n",
      "      _id         singkat                 hasil\n",
      "0        1             @                   di  \n",
      "1        2          abis               habis   \n",
      "2        3            ad                  ada  \n",
      "3        4          adlh               adalah  \n",
      "4        5         afaik     as far as i know  \n",
      "...    ...            ...                   ...\n",
      "1016  1017        wkwkkw             tertawa   \n",
      "1017  1018       ahokncc                 ahok  \n",
      "1018  1019         istaa                nista  \n",
      "1019  1020    benarjujur                jujur  \n",
      "1020  1021         mgkin               mungkin \n",
      "\n",
      "[1021 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(key_norm.dtypes)\n",
    "print(key_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f2d9c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key_norm = pd.read_csv('https://github.com/louisowen6/NLP_bahasa_resources/blob/master/combined_slang_words.txt')\n",
    "\n",
    "def text_normalize(text):\n",
    "  text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])\n",
    "  text = str.lower(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18d9aa",
   "metadata": {},
   "source": [
    "# Filtering stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "432d424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ind = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "948200cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c68c694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat fungsi untuk langkah stopword removal\n",
    "\n",
    "#more_stopword = ['barang','kirim']                    # Tambahkan kata dalam daftar stopword\n",
    "#stopwords_ind = stopwords_ind + more_stopword\n",
    "\n",
    "def remove_stop_words(text):\n",
    "  clean_words = []\n",
    "  text = text.split()\n",
    "  for word in text:\n",
    "      if word not in stopwords_ind:\n",
    "          clean_words.append(word)\n",
    "  return \" \".join(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "198929c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data\t\t:  @Dickystyawan11 @seruanhl @AgusYudhoyono Bangga rakyatnya ngantri minyak goreng,,ðŸ¤£ðŸ¤£\n",
      "Case folding\t\t:   bangga rakyatnya ngantri minyak goreng,, \n",
      "Stopword removal\t:  bangga rakyatnya ngantri minyak goreng,,\n"
     ]
    }
   ],
   "source": [
    "raw_sample = data['Text'].iloc[0]\n",
    "case_folding = casefolding(raw_sample)\n",
    "stopword_removal = remove_stop_words(case_folding)\n",
    "\n",
    "print('Raw data\\t\\t: ', raw_sample)\n",
    "print('Case folding\\t\\t: ', case_folding)\n",
    "print('Stopword removal\\t: ', stopword_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d56c04",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7c009f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Buat fungsi untuk langkah stemming bahasa Indonesia\n",
    "def stemming(text):\n",
    "  text = stemmer.stem(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c96d3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data\t\t:  @Dickystyawan11 @seruanhl @AgusYudhoyono Bangga rakyatnya ngantri minyak goreng,,ðŸ¤£ðŸ¤£\n",
      "Case folding\t\t:   bangga rakyatnya ngantri minyak goreng,, \n",
      "Stopword removal\t:  bangga rakyatnya ngantri minyak goreng,,\n",
      "Stemming\t\t:  bangga rakyat ngantri minyak goreng\n"
     ]
    }
   ],
   "source": [
    "raw_sample = data['Text'].iloc[0]\n",
    "case_folding = casefolding(raw_sample)\n",
    "stopword_removal = remove_stop_words(case_folding)\n",
    "text_stemming = stemming(stopword_removal)\n",
    "\n",
    "print('Raw data\\t\\t: ', raw_sample)\n",
    "print('Case folding\\t\\t: ', case_folding)\n",
    "print('Stopword removal\\t: ', stopword_removal)\n",
    "print('Stemming\\t\\t: ', text_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d37eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
